{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the datset from Kaggle and Unzipping the data\n"
      ],
      "metadata": {
        "id": "gxhyM6hbUvYI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eAAuFS3Sihf",
        "outputId": "d9d106ce-2e8d-419d-f016-32bb9aaab5be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.14)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"
          ]
        }
      ],
      "source": [
        "# Load dataset from kaggle and install necessary packages\n",
        "\n",
        "!pip install kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary kaggle packages\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi"
      ],
      "metadata": {
        "id": "USmSOhKnTTRe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kaggle api setip\n",
        "\n",
        "# Setting up Kaggle API credentials\n",
        "os.environ['KAGGLE_USERNAME'] = 'krishnaheroor'  #add your kaggle username\n",
        "os.environ['KAGGLE_KEY'] = 'please add the api token for above username'\n",
        "\n",
        "\n",
        "api = KaggleApi()\n",
        "api.authenticate()"
      ],
      "metadata": {
        "id": "3IHyQy98Tahw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mapping dataset from kaggle\n",
        "\n",
        "dataset = 'datasets/face-expression-recognition-dataset'\n",
        "download = 'datasets/face-expression-recognition-dataset.zip'"
      ],
      "metadata": {
        "id": "t-wxZlv2TkKs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the above dataset for project\n",
        "api.dataset_download_files('jonathanoheix/face-expression-recognition-dataset', path='datasets', unzip=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtAJQMn_UPbl",
        "outputId": "2a0aac73-7bdc-49ab-d30b-a0b5f521e8f6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/jonathanoheix/face-expression-recognition-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#unzipping the datsset\n",
        "with zipfile.ZipFile(download, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset)"
      ],
      "metadata": {
        "id": "gxkZ6jq5UY-T"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image pre-processing\n"
      ],
      "metadata": {
        "id": "3BMkbV1NU4hW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the necessary packages and libraries\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "metadata": {
        "id": "NnilrzW5U-Pv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nKfsBma3XEB9",
        "outputId": "a6eb7c53-5de5-4904-c892-9af66a9f0dbe"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'datasets/face-expression-recognition-dataset'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset preparatons\n",
        "\n",
        "train_dir = '/content/datasets/face-expression-recognition-dataset/images/train'\n",
        "val_dir = '/content/datasets/face-expression-recognition-dataset/images/validation'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuAvNriBVArj",
        "outputId": "4754b766-fe58-4061-d742-3ce045246791"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/datasets/face-expression-recognition-dataset/images/train',\n",
              " '/content/datasets/face-expression-recognition-dataset/images/validation')"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up image size/dimns\n",
        "IMG_SIZE = 48"
      ],
      "metadata": {
        "id": "UnBWUvzbVS0V"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping string labels to numeric values\n",
        "label_map = {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}"
      ],
      "metadata": {
        "id": "a1pkCgOMVhQX"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load data\n",
        "\n",
        "def load_data(data_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for label in os.listdir(data_dir):\n",
        "        class_dir = os.path.join(data_dir, label)\n",
        "        if os.path.isdir(class_dir):\n",
        "            if label in label_map:\n",
        "                numeric_label = label_map[label]\n",
        "                for img_name in os.listdir(class_dir):\n",
        "                    img_path = os.path.join(class_dir, img_name)\n",
        "                    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  #if in case gray scale images\n",
        "                    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))  #resizing into 48x48 image size\n",
        "                    img = img.astype('float32') / 255.0  #mapping between 0 to 1 as its a gray scale\n",
        "                    images.append(img)\n",
        "                    labels.append(numeric_label)\n",
        "            else:\n",
        "                print(f\"Ignoring unknown label '{label}'\")\n",
        "\n",
        "    images = np.array(images).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "    labels = to_categorical(labels, num_classes=7)  # we have 7 classes\n",
        "    return images, labels"
      ],
      "metadata": {
        "id": "Ebcm_GsDYhni"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training and validation/test data\n",
        "try:\n",
        "    X_train, y_train = load_data(train_dir)\n",
        "    X_val, y_val = load_data(val_dir)\n",
        "    print(\"Data loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff4Snzl_Y7O-",
        "outputId": "428a3686-904b-4b5d-8baa-abe85579859a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation if required\n",
        "\n",
        "# new_data_generate = ImageDataGenerator(\n",
        "#     rotation_range=10,\n",
        "#     zoom_range=0.1,\n",
        "#     width_shift_range=0.1,\n",
        "#     height_shift_range=0.1,\n",
        "#     horizontal_flip=True,\n",
        "    # vertical_flip=True,\n",
        "    # horizontal_flip = True,\n",
        "    # fill_mode='nearest'\n",
        "# )"
      ],
      "metadata": {
        "id": "FelnMXZ4ZF0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building"
      ],
      "metadata": {
        "id": "p9LOysjzZlzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
      ],
      "metadata": {
        "id": "3YdiffeAY_6s"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let initialize the model\n",
        "\n",
        "model = Sequential()"
      ],
      "metadata": {
        "id": "XNkYvoPZZsVB"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dense neural network architecture and stage creation\n",
        "\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(7, activation='softmax'))"
      ],
      "metadata": {
        "id": "nCEaavrKZxnK"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setthe model with necesasry back propogation techniques like optimizers, cost function etc..\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#cost/loss function used is - categorical_crossentropy (As we converted to whole numerical part in above stage)\n",
        "#optimizers - To achive gradient decent - Adam we used"
      ],
      "metadata": {
        "id": "TgNi319-Z7bR"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training with Hyper parameters like epochs, learning rate etc"
      ],
      "metadata": {
        "id": "S9yJ2WkvaVI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "\n",
        "\n",
        "# history_check = model.fit(\n",
        "#     new_data_generate.flow(X_train, y_train, batch_size=64),\n",
        "#     validation_data=(X_val, y_val),\n",
        "#     epochs=30\n",
        "# )   use this if you used data augmentation or else below code"
      ],
      "metadata": {
        "id": "kOgAcQTpaTrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_check = model.fit(X_train, y_train, epochs=30, batch_size=64, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XItQf4Zaruh",
        "outputId": "9eebca22-d065-4dbc-b48e-f1b115bed12c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "451/451 [==============================] - 99s 214ms/step - loss: 1.7392 - accuracy: 0.2904 - val_loss: 1.5620 - val_accuracy: 0.3889\n",
            "Epoch 2/30\n",
            "451/451 [==============================] - 89s 198ms/step - loss: 1.5286 - accuracy: 0.4097 - val_loss: 1.4457 - val_accuracy: 0.4533\n",
            "Epoch 3/30\n",
            "451/451 [==============================] - 90s 199ms/step - loss: 1.4201 - accuracy: 0.4523 - val_loss: 1.3269 - val_accuracy: 0.4956\n",
            "Epoch 4/30\n",
            "451/451 [==============================] - 89s 197ms/step - loss: 1.3505 - accuracy: 0.4866 - val_loss: 1.2786 - val_accuracy: 0.5221\n",
            "Epoch 5/30\n",
            "451/451 [==============================] - 95s 211ms/step - loss: 1.2952 - accuracy: 0.5086 - val_loss: 1.2355 - val_accuracy: 0.5355\n",
            "Epoch 6/30\n",
            "451/451 [==============================] - 95s 210ms/step - loss: 1.2408 - accuracy: 0.5300 - val_loss: 1.2182 - val_accuracy: 0.5470\n",
            "Epoch 7/30\n",
            "451/451 [==============================] - 94s 208ms/step - loss: 1.1990 - accuracy: 0.5483 - val_loss: 1.2267 - val_accuracy: 0.5304\n",
            "Epoch 8/30\n",
            "451/451 [==============================] - 89s 198ms/step - loss: 1.1667 - accuracy: 0.5581 - val_loss: 1.1827 - val_accuracy: 0.5572\n",
            "Epoch 9/30\n",
            "451/451 [==============================] - 90s 200ms/step - loss: 1.1286 - accuracy: 0.5734 - val_loss: 1.1808 - val_accuracy: 0.5618\n",
            "Epoch 10/30\n",
            "451/451 [==============================] - 95s 211ms/step - loss: 1.0977 - accuracy: 0.5857 - val_loss: 1.1575 - val_accuracy: 0.5667\n",
            "Epoch 11/30\n",
            "451/451 [==============================] - 88s 196ms/step - loss: 1.0698 - accuracy: 0.5962 - val_loss: 1.1683 - val_accuracy: 0.5582\n",
            "Epoch 12/30\n",
            "451/451 [==============================] - 90s 199ms/step - loss: 1.0370 - accuracy: 0.6059 - val_loss: 1.1610 - val_accuracy: 0.5705\n",
            "Epoch 13/30\n",
            "451/451 [==============================] - 89s 197ms/step - loss: 1.0122 - accuracy: 0.6178 - val_loss: 1.1699 - val_accuracy: 0.5693\n",
            "Epoch 14/30\n",
            "451/451 [==============================] - 90s 200ms/step - loss: 0.9764 - accuracy: 0.6308 - val_loss: 1.1764 - val_accuracy: 0.5647\n",
            "Epoch 15/30\n",
            "451/451 [==============================] - 89s 197ms/step - loss: 0.9587 - accuracy: 0.6340 - val_loss: 1.1790 - val_accuracy: 0.5729\n",
            "Epoch 16/30\n",
            "451/451 [==============================] - 94s 208ms/step - loss: 0.9261 - accuracy: 0.6490 - val_loss: 1.1903 - val_accuracy: 0.5747\n",
            "Epoch 17/30\n",
            "451/451 [==============================] - 93s 207ms/step - loss: 0.9078 - accuracy: 0.6549 - val_loss: 1.2328 - val_accuracy: 0.5727\n",
            "Epoch 18/30\n",
            "451/451 [==============================] - 88s 196ms/step - loss: 0.8805 - accuracy: 0.6642 - val_loss: 1.2580 - val_accuracy: 0.5709\n",
            "Epoch 19/30\n",
            "451/451 [==============================] - 90s 199ms/step - loss: 0.8584 - accuracy: 0.6723 - val_loss: 1.2466 - val_accuracy: 0.5754\n",
            "Epoch 20/30\n",
            "451/451 [==============================] - 88s 196ms/step - loss: 0.8389 - accuracy: 0.6814 - val_loss: 1.2531 - val_accuracy: 0.5801\n",
            "Epoch 21/30\n",
            "451/451 [==============================] - 91s 203ms/step - loss: 0.8099 - accuracy: 0.6907 - val_loss: 1.2784 - val_accuracy: 0.5668\n",
            "Epoch 22/30\n",
            "451/451 [==============================] - 91s 202ms/step - loss: 0.7919 - accuracy: 0.6916 - val_loss: 1.3071 - val_accuracy: 0.5766\n",
            "Epoch 23/30\n",
            "451/451 [==============================] - 94s 208ms/step - loss: 0.7649 - accuracy: 0.7054 - val_loss: 1.3192 - val_accuracy: 0.5785\n",
            "Epoch 24/30\n",
            "451/451 [==============================] - 94s 208ms/step - loss: 0.7452 - accuracy: 0.7116 - val_loss: 1.3301 - val_accuracy: 0.5760\n",
            "Epoch 25/30\n",
            "451/451 [==============================] - 93s 207ms/step - loss: 0.7270 - accuracy: 0.7171 - val_loss: 1.3748 - val_accuracy: 0.5681\n",
            "Epoch 26/30\n",
            "451/451 [==============================] - 89s 196ms/step - loss: 0.7112 - accuracy: 0.7207 - val_loss: 1.4074 - val_accuracy: 0.5722\n",
            "Epoch 27/30\n",
            "451/451 [==============================] - 90s 199ms/step - loss: 0.6948 - accuracy: 0.7257 - val_loss: 1.4691 - val_accuracy: 0.5726\n",
            "Epoch 28/30\n",
            "451/451 [==============================] - 89s 197ms/step - loss: 0.6860 - accuracy: 0.7314 - val_loss: 1.4352 - val_accuracy: 0.5702\n",
            "Epoch 29/30\n",
            "451/451 [==============================] - 90s 200ms/step - loss: 0.6668 - accuracy: 0.7395 - val_loss: 1.4658 - val_accuracy: 0.5791\n",
            "Epoch 30/30\n",
            "451/451 [==============================] - 98s 217ms/step - loss: 0.6496 - accuracy: 0.7460 - val_loss: 1.5251 - val_accuracy: 0.5720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model"
      ],
      "metadata": {
        "id": "QKSNnmg2lG3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('face_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFZU0zN3lIts",
        "outputId": "1dc049b5-a47e-48e3-eee8-2976636b233e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference code for testing"
      ],
      "metadata": {
        "id": "XHHghP5zldS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('face_model.h5')\n",
        "\n",
        "# Define labels for emotions\n",
        "labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "# Function to classify emotion from an image file path or web cam\n",
        "def classify_emotion_from_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Failed to load image from path: {image_path}\")\n",
        "        return None\n",
        "\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    img_resized = cv2.resize(img_gray, (48, 48))\n",
        "    img_normalized = img_resized.astype('float32') / 255.0\n",
        "    img_input = np.expand_dims(img_normalized, axis=-1)\n",
        "    img_input = np.expand_dims(img_input, axis=0)\n",
        "\n",
        "    # here we are predecting the results\n",
        "    predictions = model.predict(img_input)\n",
        "    predicted_label = np.argmax(predictions)\n",
        "\n",
        "    # converting to array of numbers fro above predictions\n",
        "    emotion = labels[predicted_label]\n",
        "\n",
        "    return emotion\n",
        "\n",
        "# only if you wnat to use webcam else comment the below code\n",
        "# cap = cv2.VideoCapture(0)\n",
        "\n",
        "# while True:\n",
        "#     # Capture frame-by-frame from webcam\n",
        "#     ret, frame = cap.read()\n",
        "\n",
        "#     if not ret:\n",
        "#         print(\"Failed to capture frame from webcam. Exiting...\")\n",
        "#         break\n",
        "\n",
        "#     # Perform emotion classification on the webcam frame\n",
        "#     predicted_emotion_webcam = classify_emotion(frame)\n",
        "\n",
        "#     # Display the webcam frame with emotion prediction\n",
        "#     cv2.putText(frame, f\"Emotion (Webcam): {predicted_emotion_webcam}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "#     cv2.imshow('Webcam Emotion Classification', frame)\n",
        "\n",
        "#     # Check if 'q' key is pressed to exit webcam loop\n",
        "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "#         break\n",
        "\n",
        "# # Release the webcam\n",
        "# cap.release()\n",
        "\n",
        "# Perform emotion classification on an image\n",
        "image_path = '/content/datasets/face-expression-recognition-dataset/images/validation/angry/10121.jpg'  #give your image path for inference testing\n",
        "predicted_emotion_image = classify_emotion_from_image(image_path)\n",
        "\n",
        "# prediction here to check the lables of specific image\n",
        "if predicted_emotion_image:\n",
        "    print(f\"Predicted Emotion (Image): {predicted_emotion_image}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRVszwexlgQO",
        "outputId": "513439fa-be05-4b93-f8cd-86f08d3ca28c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7acf6f1ef250> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 89ms/step\n",
            "Predicted Emotion (Image): Angry\n"
          ]
        }
      ]
    }
  ]
}